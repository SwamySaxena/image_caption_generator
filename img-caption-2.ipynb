{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":364443,"sourceType":"datasetVersion","datasetId":159035},{"sourceId":9522639,"sourceType":"datasetVersion","datasetId":5798162},{"sourceId":9546126,"sourceType":"datasetVersion","datasetId":5815882},{"sourceId":9546682,"sourceType":"datasetVersion","datasetId":5816295}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.137264Z","iopub.execute_input":"2024-10-04T13:27:04.137698Z","iopub.status.idle":"2024-10-04T13:27:04.144204Z","shell.execute_reply.started":"2024-10-04T13:27:04.137659Z","shell.execute_reply":"2024-10-04T13:27:04.143143Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"import string\nimport numpy as np\nimport PIL.Image\n\nfrom os import listdir\nfrom pickle import dump, load\n\nfrom numpy import array\nfrom numpy import argmax","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.146258Z","iopub.execute_input":"2024-10-04T13:27:04.147198Z","iopub.status.idle":"2024-10-04T13:27:04.157482Z","shell.execute_reply.started":"2024-10-04T13:27:04.147149Z","shell.execute_reply":"2024-10-04T13:27:04.156264Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"image_dir = '/kaggle/input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset'\ntext_dir = '/kaggle/input/flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt'","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.159512Z","iopub.execute_input":"2024-10-04T13:27:04.159846Z","iopub.status.idle":"2024-10-04T13:27:04.172462Z","shell.execute_reply.started":"2024-10-04T13:27:04.159811Z","shell.execute_reply":"2024-10-04T13:27:04.171513Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"def load_description(text):\n    mapping = dict()\n    for line in text.split(\"\\n\"): \n        token = line.split(\"\\t\")\n        if len(line) < 2:   # remove short descriptions \n            continue\n        img_id = token[0].split('.')[0] # name of the image \n        img_des = token[1]              # description of the image \n        if img_id not in mapping: \n            mapping[img_id] = list() \n        mapping[img_id].append(img_des) \n    return mapping ","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.173609Z","iopub.execute_input":"2024-10-04T13:27:04.173948Z","iopub.status.idle":"2024-10-04T13:27:04.183689Z","shell.execute_reply.started":"2024-10-04T13:27:04.173914Z","shell.execute_reply":"2024-10-04T13:27:04.182720Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"token_path = '/kaggle/input/flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt'\ntext = open(token_path, 'r', encoding = 'utf-8').read() \ndescriptions = load_description(text) \nprint(descriptions['1000268201_693b08cb0e'])","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.186427Z","iopub.execute_input":"2024-10-04T13:27:04.186846Z","iopub.status.idle":"2024-10-04T13:27:04.275641Z","shell.execute_reply.started":"2024-10-04T13:27:04.186800Z","shell.execute_reply":"2024-10-04T13:27:04.274470Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_description(desc):\n    for key, des_list in desc.items(): \n        for i in range(len(des_list)): \n            caption = des_list[i] \n            caption = [ch for ch in caption if ch not in string.punctuation]  # remove punctuations\n            caption = ''.join(caption) #back to string\n            caption = caption.split(' ') #remove spaces\n            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]  #convert to lowercase\n            caption = ' '.join(caption)\n            des_list[i] = caption \n            ","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.276958Z","iopub.execute_input":"2024-10-04T13:27:04.277309Z","iopub.status.idle":"2024-10-04T13:27:04.284738Z","shell.execute_reply.started":"2024-10-04T13:27:04.277273Z","shell.execute_reply":"2024-10-04T13:27:04.283494Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"clean_description(descriptions) \ndescriptions['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.285930Z","iopub.execute_input":"2024-10-04T13:27:04.286247Z","iopub.status.idle":"2024-10-04T13:27:04.872388Z","shell.execute_reply.started":"2024-10-04T13:27:04.286215Z","shell.execute_reply":"2024-10-04T13:27:04.871391Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"['child in pink dress is climbing up set of stairs in an entry way',\n 'girl going into wooden building',\n 'little girl climbing into wooden playhouse',\n 'little girl climbing the stairs to her playhouse',\n 'little girl in pink dress going into wooden cabin']"},"metadata":{}}]},{"cell_type":"code","source":"def to_vocab(desc): \n    words = set() \n    for key in desc.keys(): \n        for line in desc[key]: \n            words.update(line.split()) \n    return words ","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.873709Z","iopub.execute_input":"2024-10-04T13:27:04.874102Z","iopub.status.idle":"2024-10-04T13:27:04.879437Z","shell.execute_reply.started":"2024-10-04T13:27:04.874066Z","shell.execute_reply":"2024-10-04T13:27:04.878467Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"vocab = to_vocab(descriptions)\nprint(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.880723Z","iopub.execute_input":"2024-10-04T13:27:04.881105Z","iopub.status.idle":"2024-10-04T13:27:04.950832Z","shell.execute_reply.started":"2024-10-04T13:27:04.881061Z","shell.execute_reply":"2024-10-04T13:27:04.949805Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"8763\n","output_type":"stream"}]},{"cell_type":"code","source":"#time to split train and test\nimport glob \nimages = '/kaggle/input/flicker8k-dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n# Create a list of all image names in the directory \nimg = glob.glob(images + '*.jpg')\n\ntrain_path = '/kaggle/input/flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\ntrain_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\") \ntrain_img = []  # list of all images in training set \nfor im in img: \n    if(im[len(images):] in train_images): \n        train_img.append(im) \n        \n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:04.952073Z","iopub.execute_input":"2024-10-04T13:27:04.952402Z","iopub.status.idle":"2024-10-04T13:27:05.923710Z","shell.execute_reply.started":"2024-10-04T13:27:04.952368Z","shell.execute_reply":"2024-10-04T13:27:05.922841Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# load descriptions of training set in a dictionary. Name of the image will act as ey \ndef load_clean_descriptions(des, dataset): \n    dataset_des = dict() \n    for key, des_list in des.items(): \n        if key+'.jpg' in dataset: \n            if key not in dataset_des: \n                dataset_des[key] = list() \n            for line in des_list: \n                desc = 'startseq ' + line + ' endseq'\n                dataset_des[key].append(desc) \n    return dataset_des ","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:05.927938Z","iopub.execute_input":"2024-10-04T13:27:05.928305Z","iopub.status.idle":"2024-10-04T13:27:05.934990Z","shell.execute_reply.started":"2024-10-04T13:27:05.928268Z","shell.execute_reply":"2024-10-04T13:27:05.933867Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"train_descriptions = load_clean_descriptions(descriptions, train_images) \nprint(train_descriptions['1000268201_693b08cb0e'])","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:05.936482Z","iopub.execute_input":"2024-10-04T13:27:05.936798Z","iopub.status.idle":"2024-10-04T13:27:06.854446Z","shell.execute_reply.started":"2024-10-04T13:27:05.936764Z","shell.execute_reply":"2024-10-04T13:27:06.853399Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"['startseq child in pink dress is climbing up set of stairs in an entry way endseq', 'startseq girl going into wooden building endseq', 'startseq little girl climbing into wooden playhouse endseq', 'startseq little girl climbing the stairs to her playhouse endseq', 'startseq little girl in pink dress going into wooden cabin endseq']\n","output_type":"stream"}]},{"cell_type":"code","source":"# list of all training captions \nall_train_captions = [] \nfor key, val in train_descriptions.items(): \n    for caption in val: \n        all_train_captions.append(caption) \n        \n# consider only words which occur atleast 10 times \nvocabulary = vocab \nthreshold = 10 # you can change this value according to your need \nword_counts = {} \nfor cap in all_train_captions: \n    for word in cap.split(' '): \n        word_counts[word] = word_counts.get(word, 0) + 1\n  \nvocab = [word for word in word_counts if word_counts[word] >= threshold] \nvocab_size = len(vocab) + 1","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:06.855917Z","iopub.execute_input":"2024-10-04T13:27:06.856354Z","iopub.status.idle":"2024-10-04T13:27:07.032822Z","shell.execute_reply.started":"2024-10-04T13:27:06.856307Z","shell.execute_reply":"2024-10-04T13:27:07.032000Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"print(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:07.033906Z","iopub.execute_input":"2024-10-04T13:27:07.034243Z","iopub.status.idle":"2024-10-04T13:27:07.039194Z","shell.execute_reply.started":"2024-10-04T13:27:07.034209Z","shell.execute_reply":"2024-10-04T13:27:07.038300Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"1652\n","output_type":"stream"}]},{"cell_type":"code","source":"# word mapping to integers \nixtoword = {} \nwordtoix = {} \n  \nix = 1\nfor word in vocab: \n    wordtoix[word] = ix \n    ixtoword[ix] = word \n    ix += 1\n      \n# find the maximum length of a description in a dataset \nmax_length = max(len(des.split()) for des in all_train_captions) \nmax_length","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:07.040417Z","iopub.execute_input":"2024-10-04T13:27:07.040804Z","iopub.status.idle":"2024-10-04T13:27:07.079940Z","shell.execute_reply.started":"2024-10-04T13:27:07.040758Z","shell.execute_reply":"2024-10-04T13:27:07.079035Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"34"},"metadata":{}}]},{"cell_type":"code","source":"#extract image features\ndef load_image(image_path, max_size = 224):\n    image = Image.open(image_path).convert('RGB')\n    \n    size = max_size if max(image.size) > max_size else max(image.size)\n    \n    transformations = transforms.Compose([\n        transforms.Resize(size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225)) \n    ])\n    \n    image = transformations(image)[:3, :, :].unsqueeze(0)\n    return image.to(device)\n\ndef extract_features(directory = image_dir):\n    model =  models.vgg19(pretrained=True).to(device)\n    model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n    model.eval()\n    features = dict()\n    with torch.no_grad():\n        for name in listdir(directory):\n            filename = directory + '/' + name\n            image = load_image(filename, 224)\n            image.to(device)\n            feature = model(image)\n            image_id = name.split('.')[0]\n\n            features[image_id] = feature\n            torch.cuda.empty_cache()\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:07.080939Z","iopub.execute_input":"2024-10-04T13:27:07.081212Z","iopub.status.idle":"2024-10-04T13:27:07.091192Z","shell.execute_reply.started":"2024-10-04T13:27:07.081184Z","shell.execute_reply":"2024-10-04T13:27:07.090173Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"features = extract_features()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:27:07.092244Z","iopub.execute_input":"2024-10-04T13:27:07.092560Z","iopub.status.idle":"2024-10-04T13:30:19.973099Z","shell.execute_reply.started":"2024-10-04T13:27:07.092526Z","shell.execute_reply":"2024-10-04T13:30:19.972142Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"def get_train_features():\n    train = {}\n    for key, feat in features.items():\n        key = key + \".jpg\"\n        if key in train_images:\n            train[key] = feat\n    return train","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:30:19.975487Z","iopub.execute_input":"2024-10-04T13:30:19.976453Z","iopub.status.idle":"2024-10-04T13:30:19.981809Z","shell.execute_reply.started":"2024-10-04T13:30:19.976403Z","shell.execute_reply":"2024-10-04T13:30:19.980711Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train_features = get_train_features()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:30:19.983161Z","iopub.execute_input":"2024-10-04T13:30:19.983490Z","iopub.status.idle":"2024-10-04T13:30:20.873228Z","shell.execute_reply.started":"2024-10-04T13:30:19.983454Z","shell.execute_reply":"2024-10-04T13:30:20.872207Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"def tto_categorical(y, num_classes):\n    \"\"\" 1-hot encodes a tensor \"\"\"\n    return np.eye(num_classes, dtype='uint8')[y]","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:30:20.875675Z","iopub.execute_input":"2024-10-04T13:30:20.876563Z","iopub.status.idle":"2024-10-04T13:30:20.881366Z","shell.execute_reply.started":"2024-10-04T13:30:20.876513Z","shell.execute_reply":"2024-10-04T13:30:20.880368Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef pad_sequences(sequences, maxlen=None, padding_value=0):\n    # Check the maximum length if not provided\n    if maxlen is None:\n        maxlen = max(len(seq) for seq in sequences)\n\n    # Create a tensor filled with the padding value\n    padded_sequences = torch.full((len(sequences), maxlen), padding_value)\n\n    # Pad each sequence\n    for i, seq in enumerate(sequences):\n        length = len(seq)\n        # Copy the sequence into the padded tensor\n        padded_sequences[i, :length] = torch.tensor(seq)\n\n    return padded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:30:20.882718Z","iopub.execute_input":"2024-10-04T13:30:20.883089Z","iopub.status.idle":"2024-10-04T13:30:20.892367Z","shell.execute_reply.started":"2024-10-04T13:30:20.883013Z","shell.execute_reply":"2024-10-04T13:30:20.891151Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"X1, X2, y = list(), list(), list() \nfor key, des_list in train_descriptions.items(): \n    pic = train_features[key + '.jpg'] \n    if pic.is_cuda:  # Check if it's a GPU tensor\n        pic = pic.cpu()\n        \n    for cap in des_list: \n        seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix] \n        for i in range(1, len(seq)): \n            in_seq, out_seq = seq[:i], seq[i] \n            \n            in_seq = pad_sequences([in_seq], maxlen = max_length)[0] \n            out_seq = tto_categorical([out_seq], num_classes = vocab_size)[0] \n            # store \n            X1.append(pic) \n            X2.append(in_seq) \n            y.append(out_seq) \n  \nX2 = np.array(X2) \nX1 = np.array([pic.cpu().numpy() if torch.is_tensor(pic) else pic for pic in X1])  # Ensure all elements are NumPy arrays \ny = np.array(y) ","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:30:20.893649Z","iopub.execute_input":"2024-10-04T13:30:20.894045Z","iopub.status.idle":"2024-10-04T13:31:25.412446Z","shell.execute_reply.started":"2024-10-04T13:30:20.893998Z","shell.execute_reply":"2024-10-04T13:31:25.411364Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Convert NumPy arrays to PyTorch tensors\nX1 = torch.from_numpy(X1).float()  # Ensure floating-point tensor for image features\nX2 = torch.from_numpy(X2).float()   # Integer type for captions (word indices)\ny = torch.from_numpy(y).float()     # Integer type for targets (next word index)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:25.415392Z","iopub.execute_input":"2024-10-04T13:31:25.415712Z","iopub.status.idle":"2024-10-04T13:31:26.200073Z","shell.execute_reply.started":"2024-10-04T13:31:25.415681Z","shell.execute_reply":"2024-10-04T13:31:26.199152Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = X1.view(-1, 4096)  \nprint(X1.shape)  # Should be (num_samples, 2048)\nprint(X2.shape)  # Should be (num_samples, max_length)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:26.201492Z","iopub.execute_input":"2024-10-04T13:31:26.202121Z","iopub.status.idle":"2024-10-04T13:31:26.209256Z","shell.execute_reply.started":"2024-10-04T13:31:26.202072Z","shell.execute_reply":"2024-10-04T13:31:26.208052Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"torch.Size([292328, 4096])\ntorch.Size([292328, 34])\ntorch.Size([292328, 1652])\n","output_type":"stream"}]},{"cell_type":"code","source":"# load glove vectors for embedding layer \nembeddings_index = {} \ngolve_path ='/kaggle/input/glove-img/glove.6B.200d.txt'\nglove = open(golve_path, 'r', encoding = 'utf-8').read() \nfor line in glove.split(\"\\n\"): \n    values = line.split(\" \") \n    word = values[0] \n    indices = np.asarray(values[1: ], dtype = 'float32') \n    embeddings_index[word] = indices \n  \nemb_dim = 200\nemb_matrix = np.zeros((vocab_size, emb_dim)) \nfor word, i in wordtoix.items(): \n    emb_vec = embeddings_index.get(word) \n    if emb_vec is not None: \n        emb_matrix[i] = emb_vec \nemb_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:26.211032Z","iopub.execute_input":"2024-10-04T13:31:26.211454Z","iopub.status.idle":"2024-10-04T13:31:53.368655Z","shell.execute_reply.started":"2024-10-04T13:31:26.211406Z","shell.execute_reply":"2024-10-04T13:31:53.367565Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"(1652, 200)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nclass ImageCaptioningModel(nn.Module):\n    def __init__(self, vocab_size, emb_dim, max_length, emb_matrix):\n        super(ImageCaptioningModel, self).__init__()\n        \n        # Feature vector (Image) pathway\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc1 = nn.Linear(4096, 256)  # Dense(256)\n\n        # Sequence (Caption) pathway\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)  # Embedding layer\n        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix, dtype=torch.float32))  # Set embedding weights\n        self.embedding.weight.requires_grad = False  # Freeze embedding layer\n        \n        self.dropout2 = nn.Dropout(0.2)\n        self.lstm = nn.LSTM(emb_dim, 256, batch_first=True)  # LSTM layer\n        \n        # Decoder layers\n        self.fc2 = nn.Linear(256, 256)  # Decoder Dense(256)\n        self.fc3 = nn.Linear(256, vocab_size)  # Output layer Dense(vocab_size)\n\n    def forward(self, img_feat, caption):\n        caption = caption.long()\n        # Image feature pathway\n        x1 = self.dropout1(img_feat)\n        x1 = torch.relu(self.fc1(x1))  # Shape: (batch_size, 256)\n\n        # Caption pathway\n        x2 = self.embedding(caption)  # Shape: (batch_size, max_length, emb_dim)\n        x2 = self.dropout2(x2)\n        x2, _ = self.lstm(x2)  # Shape: (batch_size, max_length, 256)\n        x2 = x2[:, -1, :]  # Get the last time step (batch_size, 256)\n\n        # Combine features\n        combined = x1 + x2  # Add the image and caption vectors\n\n        # Decoder layers\n        x = torch.relu(self.fc2(combined))  # Dense(256)\n        output = torch.softmax(self.fc3(x), dim=1)  # Dense(vocab_size) with softmax\n        \n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:53.373646Z","iopub.execute_input":"2024-10-04T13:31:53.373965Z","iopub.status.idle":"2024-10-04T13:31:53.385885Z","shell.execute_reply.started":"2024-10-04T13:31:53.373932Z","shell.execute_reply":"2024-10-04T13:31:53.384633Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# Loss and optimizer\nmodel = ImageCaptioningModel(vocab_size, emb_dim, max_length, emb_matrix)\ncriterion = nn.CrossEntropyLoss()  # Cross-entropy loss (for classification)\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n\n# Dataset and DataLoader for batching\ndataset = TensorDataset(X1, X2, y)\ndata_loader = DataLoader(dataset, batch_size=256, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:53.387409Z","iopub.execute_input":"2024-10-04T13:31:53.387799Z","iopub.status.idle":"2024-10-04T13:31:53.561103Z","shell.execute_reply.started":"2024-10-04T13:31:53.387757Z","shell.execute_reply":"2024-10-04T13:31:53.560032Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"def to_one_hot(y, num_classes):\n    predicted_classes = torch.argmax(y, dim=1)  # Shape will be (256,)\n    num_classes = y.size(1)  # Get the number of classes (1652 in this case)\n    one_hot = torch.zeros_like(y)  # Create a tensor of zeros with the same shape as logits\n    one_hot[torch.arange(y.size(0)), predicted_classes] = 1  # Set the appropriate indices to 1\n\n    return one_hot","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:53.562547Z","iopub.execute_input":"2024-10-04T13:31:53.563076Z","iopub.status.idle":"2024-10-04T13:31:53.569744Z","shell.execute_reply.started":"2024-10-04T13:31:53.563028Z","shell.execute_reply":"2024-10-04T13:31:53.568613Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n# Training loop\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    \n    for img_feat, caption, target in tqdm(data_loader):\n        optimizer.zero_grad()  # Clear gradients\n        #print(img_feat)\n        output = model(img_feat, caption)  # Forward pass\n        target = target.float()  # If your target is one-hot encoded\n        \n        # Calculate loss using Categorical Cross Entropy\n        loss = F.binary_cross_entropy_with_logits(output, target)\n        \n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update weights\n        \n        total_loss += loss.item()\n\n    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(data_loader):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T13:31:53.571155Z","iopub.execute_input":"2024-10-04T13:31:53.571696Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 1142/1142 [03:51<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.6934\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1142/1142 [03:55<00:00,  4.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Loss: 0.6934\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 940/1142 [03:12<00:39,  5.08it/s]","output_type":"stream"}]},{"cell_type":"code","source":"def greedy_search(pic): \n    start = 'startseq'\n    pic = extract_features(pic).items()\n    \n    # Extract features and assume the first value is the tensor\n    for key, val in pic:\n        pic = val\n    \n    # Move the pic tensor to the same device as the model\n    pic = pic.to(next(model.parameters()).device)  # This gets the device of the model parameters\n    \n    for i in range(max_length): \n        seq = [wordtoix[word] for word in start.split() if word in wordtoix] \n        seq = pad_sequences([seq], maxlen=max_length) \n        \n        # Convert seq to tensor and move it to the same device as the model\n        seq = torch.tensor(seq, device=next(model.parameters()).device)  # Ensure seq is on the same device\n        \n        yhat = model(pic, seq)\n\n        # Detach yhat from the computation graph and convert to NumPy\n        yhat = yhat.detach().cpu().numpy()  # Ensure to detach and move to CPU for numpy operation\n        \n        print(\"Model Output:\", yhat)  # Debugging line\n\n        yhat = np.argmax(yhat)  # Now you can safely use np.argmax\n        print(\"Predicted Index:\", yhat)  # Debugging line\n        \n        word = ixtoword[yhat] \n        print(\"Predicted Word:\", word)  # Debugging line\n\n        start += ' ' + word \n        \n        if word == 'endseq': \n            print(\"Early termination with word:\", word)  # Debugging line\n            break\n    \n    final = start.split() \n    print(\"Generated Sequence:\", final)  # Debugging line\n    final = final[1:-1] \n    final = ' '.join(final) \n    return final\n\n# Usage\noutput = greedy_search('/kaggle/input/sample-image')\nprint(\"Length of Output:\", len(output))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = greedy_search('/kaggle/input/randomimg/')\nprint(len(output))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}